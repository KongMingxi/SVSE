###########################
# Model
###########################

# Path to model weights (for initialization)
backbone: resnet18
backbone_pretrained: True
init_weights: ""

###########################
# Optimization
###########################
optim: "adam"
optim_lr: 0.0001
optim_weight_decay: 5e-4
optim_momentum: 0.9
# hyperparameters for SGD
optim_sgd_dampning: 0
optim_sgd_nesterov: False
optim_rmsprop_alpha: 0.99
# the following also apply to other
# adaptive optimizers like adamw
optim_adam_beta1: 0.9
optim_adam_beta2: 0.999
# staged_lr allows different layers to have
# different lr, e.g. pre-trained base layers
# can be assigned a smaller lr than the new
# classification layer
optim_staged_lr: False
optim_new_layers: ()
optim_base_lr_mult: 0.1
# learning rate scheduler
optim_lr_scheduler: "single_step"
# -1 or 0 means the stepsize is equal to max_epoch
optim_stepsize: (-1, )
optim_gamma: 0.1
optim_max_epoch: 50
# set warmup_epoch larger than 0 to activate warmup training
optim_warmup_epoch: -1
# either linear or constant
optim_warmup_type: "linear"
# constant learning rate when type=constant
optim_warmup_cons_lr: 1e-5
# minimum learning rate when type=linear
optim_warmup_min_lr: 1e-5
# recount epoch for the next scheduler (last_epoch=-1)
# otherwise last_epoch=warmup_epoch
optim_warmup_recount: True